{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# used to look at images\n",
    "from skimage import io, color, filters, feature\n",
    "from skimage.transform import resize, rotate\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# to read in the .mat files\n",
    "import scipy.io as sio \n",
    "\n",
    "\n",
    "# tensorflow - for CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# used to import the image folders\n",
    "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used the train_list.mat file to create dictionary of label names and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = sio.loadmat('extras/lists/train_list.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to create list of breed names from train_dict\n",
    "breeds = []\n",
    "for i in np.unique(train_dict['labels']):\n",
    "    # get one file - first 0 is just first file with the label\n",
    "    # second 0 is just the file path\n",
    "    file = train_dict['file_list'][train_dict['labels'] == i][0][0]\n",
    "    # regex pattern\n",
    "    # n[0-9]+- denotes a pattern that starts with n, has the group 0-9 an unknown amount of times and a hyphen\n",
    "    # OR a pattern that starts with non-whitespace (\\W for the forward slash), n[0-9]+, an underscore\n",
    "    # [0-9] some amount of times and ends with .jpg\n",
    "    pattern = re.compile(r'n[0-9]+-|\\Wn[0-9]+_[0-9]+.jpg')\n",
    "    # replace this pattern for each file with an empty string and append to breeds list\n",
    "    breeds.append(pattern.sub('', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty dictionary for labels\n",
    "breed_key = dict()\n",
    "for i in np.unique(train_dict['labels']):\n",
    "    # labels start at 1, so to match python idx, subtract 1 from breeds\n",
    "    breed_key[i] = breeds[i-1]\n",
    "    \n",
    "# breed_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the images and place them into the appropriate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path1 = 'extras/Images/train1'\n",
    "train_path2 = 'extras/Images/train2'\n",
    "train_path3 = 'extras/Images/train3'\n",
    "train_path4 = 'extras/Images/train4'\n",
    "train_path5 = 'extras/Images/train5'\n",
    "# train_path = 'extras/Images_copy/train'\n",
    "\n",
    "valid_path = 'extras/Images/valid'\n",
    "test_path = 'extras/Images/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning model\n",
    "inceptionresnetv2_model = tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers that have already been trained\n",
    "for layer in inceptionresnetv2_model.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new model on top\n",
    "inputs = Input(shape = (224, 224, 3))\n",
    "x = inceptionresnetv2_model(inputs, training = False)\n",
    "# converts features of shape of model's amount of rows to vectors\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# create expected output layer - final prediction layer (dense) to predict the 120 classes\n",
    "output = Dense(120, activation = 'softmax')(x)\n",
    "\n",
    "# put it all together\n",
    "analyzer_model_preprocess = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               491640    \n",
      "=================================================================\n",
      "Total params: 134,752,184\n",
      "Trainable params: 17,272,952\n",
      "Non-trainable params: 117,479,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "analyzer_model_preprocess.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3054 images belonging to 120 classes.\n",
      "Found 3009 images belonging to 120 classes.\n",
      "Found 3009 images belonging to 120 classes.\n",
      "Found 3009 images belonging to 120 classes.\n",
      "Found 3009 images belonging to 120 classes.\n",
      "Found 272 images belonging to 120 classes.\n",
      "Found 274 images belonging to 120 classes.\n",
      "Found 274 images belonging to 120 classes.\n",
      "Found 274 images belonging to 120 classes.\n",
      "Found 274 images belonging to 120 classes.\n",
      "Found 4122 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augementation used for training, testing, and validation data\n",
    "# consider changing the pixels to be 0-1 in ImageDataGenerator\n",
    "datagen = image.ImageDataGenerator()\n",
    "datagen_training = image.ImageDataGenerator(validation_split=0.1, preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "# train_batch = datagen.flow_from_directory(train_path, target_size = (224, 224),\n",
    "#                                           classes = breeds, batch_size = 5)\n",
    "\n",
    "\n",
    "# Make smaller training batches for sake of computer\n",
    "train_batch1 = datagen_training.flow_from_directory(train_path1, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'training', seed = 42)\n",
    "train_batch2 = datagen_training.flow_from_directory(train_path2, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'training', seed = 42)\n",
    "train_batch3 = datagen_training.flow_from_directory(train_path3, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'training', seed = 42)\n",
    "train_batch4 = datagen_training.flow_from_directory(train_path4, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'training', seed = 42)\n",
    "train_batch5 = datagen_training.flow_from_directory(train_path5, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5,\n",
    "                                          subset = 'training', seed = 42)\n",
    "\n",
    "# respective validation batches of training batches\n",
    "valid_batch1 = datagen_training.flow_from_directory(train_path1, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'validation', seed = 42)\n",
    "valid_batch2 = datagen_training.flow_from_directory(train_path2, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'validation', seed = 42)\n",
    "valid_batch3 = datagen_training.flow_from_directory(train_path3, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'validation', seed = 42)\n",
    "valid_batch4 = datagen_training.flow_from_directory(train_path4, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5, \n",
    "                                          subset = 'validation', seed = 42)\n",
    "valid_batch5 = datagen_training.flow_from_directory(train_path5, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5,\n",
    "                                          subset = 'validation', seed = 42)\n",
    "\n",
    "test_batch = datagen.flow_from_directory(test_path, target_size = (224, 224),\n",
    "                                        classes = breeds, batch_size = 5)# transfer learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "- Change learning rate to 0.001?\n",
    "- Change CNN Model - InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "# used gradient descent as optimizer\n",
    "optimizer = Adam(lr = 0.0001)\n",
    "analyzer_model_preprocess.compile(optimizer = optimizer,\n",
    "                      loss = 'categorical_crossentropy',\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "# Stops training if validation loss doesn't improve\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "611/611 [==============================] - 626s 1s/step - loss: 3.8281 - accuracy: 0.2782 - val_loss: 1.1136 - val_accuracy: 0.7831\n",
      "Epoch 2/8\n",
      "611/611 [==============================] - 701s 1s/step - loss: 0.7661 - accuracy: 0.8596 - val_loss: 0.7294 - val_accuracy: 0.8015\n",
      "Epoch 3/8\n",
      "611/611 [==============================] - 707s 1s/step - loss: 0.4662 - accuracy: 0.8751 - val_loss: 0.6719 - val_accuracy: 0.8051\n",
      "Epoch 4/8\n",
      "611/611 [==============================] - 626s 1s/step - loss: 0.3575 - accuracy: 0.8977 - val_loss: 0.6656 - val_accuracy: 0.8125\n",
      "Epoch 5/8\n",
      "611/611 [==============================] - 525s 858ms/step - loss: 0.2993 - accuracy: 0.9130 - val_loss: 0.6664 - val_accuracy: 0.8051\n",
      "Epoch 6/8\n",
      "611/611 [==============================] - 467s 764ms/step - loss: 0.2684 - accuracy: 0.9192 - val_loss: 0.6889 - val_accuracy: 0.7941\n",
      "Epoch 7/8\n",
      "611/611 [==============================] - 496s 812ms/step - loss: 0.2321 - accuracy: 0.9294 - val_loss: 0.6840 - val_accuracy: 0.8051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b5733ed90>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 32 is good starting point\n",
    "# steps_per_epoch = training_size / batch_size\n",
    "analyzer_model_preprocess.fit(train_batch1, validation_data = valid_batch1, epochs = 8, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "602/602 [==============================] - 484s 804ms/step - loss: 0.5843 - accuracy: 0.8355 - val_loss: 0.5609 - val_accuracy: 0.8467\n",
      "Epoch 2/8\n",
      "602/602 [==============================] - 434s 721ms/step - loss: 0.4660 - accuracy: 0.8548 - val_loss: 0.5524 - val_accuracy: 0.8431\n",
      "Epoch 3/8\n",
      "602/602 [==============================] - 434s 721ms/step - loss: 0.3981 - accuracy: 0.8744 - val_loss: 0.5431 - val_accuracy: 0.8467\n",
      "Epoch 4/8\n",
      "602/602 [==============================] - 432s 718ms/step - loss: 0.3455 - accuracy: 0.8843 - val_loss: 0.5222 - val_accuracy: 0.8431\n",
      "Epoch 5/8\n",
      "602/602 [==============================] - 467s 775ms/step - loss: 0.3048 - accuracy: 0.8933 - val_loss: 0.5596 - val_accuracy: 0.8321\n",
      "Epoch 6/8\n",
      "602/602 [==============================] - 440s 731ms/step - loss: 0.2723 - accuracy: 0.9086 - val_loss: 0.5681 - val_accuracy: 0.8248\n",
      "Epoch 7/8\n",
      "602/602 [==============================] - 421s 700ms/step - loss: 0.2456 - accuracy: 0.9172 - val_loss: 0.5833 - val_accuracy: 0.8321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9aad4807d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_model_preprocess.fit(train_batch2, validation_data = valid_batch2, epochs = 8, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "602/602 [==============================] - 419s 696ms/step - loss: 0.5592 - accuracy: 0.8465 - val_loss: 0.6847 - val_accuracy: 0.8285\n",
      "Epoch 2/8\n",
      "602/602 [==============================] - 417s 693ms/step - loss: 0.4454 - accuracy: 0.8681 - val_loss: 0.6486 - val_accuracy: 0.8321\n",
      "Epoch 3/8\n",
      "602/602 [==============================] - 416s 691ms/step - loss: 0.3802 - accuracy: 0.8860 - val_loss: 0.6564 - val_accuracy: 0.8358\n",
      "Epoch 4/8\n",
      "602/602 [==============================] - 411s 683ms/step - loss: 0.3281 - accuracy: 0.8993 - val_loss: 0.6868 - val_accuracy: 0.8321\n",
      "Epoch 5/8\n",
      "602/602 [==============================] - 411s 683ms/step - loss: 0.2937 - accuracy: 0.9059 - val_loss: 0.7027 - val_accuracy: 0.8321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b45017c10>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_model_preprocess.fit(train_batch3, validation_data = valid_batch3, epochs = 8, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "602/602 [==============================] - 413s 686ms/step - loss: 0.5336 - accuracy: 0.8415 - val_loss: 0.4425 - val_accuracy: 0.8650\n",
      "Epoch 2/8\n",
      "602/602 [==============================] - 410s 681ms/step - loss: 0.4334 - accuracy: 0.8594 - val_loss: 0.4251 - val_accuracy: 0.8686\n",
      "Epoch 3/8\n",
      "602/602 [==============================] - 410s 682ms/step - loss: 0.3713 - accuracy: 0.8714 - val_loss: 0.4331 - val_accuracy: 0.8577\n",
      "Epoch 4/8\n",
      "602/602 [==============================] - 413s 686ms/step - loss: 0.3294 - accuracy: 0.8827 - val_loss: 0.4454 - val_accuracy: 0.8577\n",
      "Epoch 5/8\n",
      "602/602 [==============================] - 414s 687ms/step - loss: 0.2946 - accuracy: 0.9006 - val_loss: 0.4418 - val_accuracy: 0.8577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b1a58d350>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_model_preprocess.fit(train_batch4, validation_data = valid_batch4, epochs = 8, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "602/602 [==============================] - 413s 686ms/step - loss: 0.5507 - accuracy: 0.8481 - val_loss: 0.4514 - val_accuracy: 0.8504\n",
      "Epoch 2/8\n",
      "602/602 [==============================] - 413s 686ms/step - loss: 0.4382 - accuracy: 0.8691 - val_loss: 0.4025 - val_accuracy: 0.8723\n",
      "Epoch 3/8\n",
      "602/602 [==============================] - 412s 684ms/step - loss: 0.3743 - accuracy: 0.8790 - val_loss: 0.4329 - val_accuracy: 0.8796\n",
      "Epoch 4/8\n",
      "602/602 [==============================] - 410s 682ms/step - loss: 0.3266 - accuracy: 0.8917 - val_loss: 0.4318 - val_accuracy: 0.8577\n",
      "Epoch 5/8\n",
      "602/602 [==============================] - 411s 683ms/step - loss: 0.2914 - accuracy: 0.9050 - val_loss: 0.4300 - val_accuracy: 0.8759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9b45017610>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_model_preprocess.fit(train_batch5, validation_data = valid_batch5, epochs = 8, callbacks = callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_TF2.0]",
   "language": "python",
   "name": "conda-env-py3_TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
