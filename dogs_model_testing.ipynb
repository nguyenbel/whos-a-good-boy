{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# used to look at images\n",
    "from skimage import io, color, filters, feature\n",
    "from skimage.transform import resize, rotate\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# to read in the .mat files\n",
    "import scipy.io as sio \n",
    "\n",
    "\n",
    "# tensorflow - for CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "# used to import the image folders\n",
    "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used the train_list.mat file to create dictionary of label names and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = sio.loadmat('extras/lists/train_list.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to create list of breed names from train_dict\n",
    "breeds = []\n",
    "for i in np.unique(train_dict['labels']):\n",
    "    # get one file - first 0 is just first file with the label\n",
    "    # second 0 is just the file path\n",
    "    file = train_dict['file_list'][train_dict['labels'] == i][0][0]\n",
    "    # regex pattern\n",
    "    # n[0-9]+- denotes a pattern that starts with n, has the group 0-9 an unknown amount of times and a hyphen\n",
    "    # OR a pattern that starts with non-whitespace (\\W for the forward slash), n[0-9]+, an underscore\n",
    "    # [0-9] some amount of times and ends with .jpg\n",
    "    pattern = re.compile(r'n[0-9]+-|\\Wn[0-9]+_[0-9]+.jpg')\n",
    "    # replace this pattern for each file with an empty string and append to breeds list\n",
    "    breeds.append(pattern.sub('', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty dictionary for labels\n",
    "breed_key = dict()\n",
    "for i in np.unique(train_dict['labels']):\n",
    "    # labels start at 1, so to match python idx, subtract 1 from breeds\n",
    "    breed_key[i] = breeds[i-1]\n",
    "    \n",
    "# breed_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the images and place them into the appropriate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path1 = 'extras/Images/train1'\n",
    "train_path2 = 'extras/Images/train2'\n",
    "train_path3 = 'extras/Images/train3'\n",
    "train_path4 = 'extras/Images/train4'\n",
    "train_path5 = 'extras/Images/train5'\n",
    "\n",
    "valid_path = 'extras/Images/valid'\n",
    "test_path = 'extras/Images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2469 images belonging to 120 classes.\n",
      "Found 2469 images belonging to 120 classes.\n",
      "Found 2469 images belonging to 120 classes.\n",
      "Found 2469 images belonging to 120 classes.\n",
      "Found 2469 images belonging to 120 classes.\n",
      "Found 4113 images belonging to 120 classes.\n",
      "Found 4122 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augementation used for training, testing, and validation data\n",
    "datagen = image.ImageDataGenerator()\n",
    "\n",
    "train_batch1 = datagen.flow_from_directory(train_path1, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5)\n",
    "train_batch2 = datagen.flow_from_directory(train_path2, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5)\n",
    "train_batch3 = datagen.flow_from_directory(train_path3, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5)\n",
    "train_batch4 = datagen.flow_from_directory(train_path4, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5)\n",
    "train_batch5 = datagen.flow_from_directory(train_path5, target_size = (224, 224),\n",
    "                                          classes = breeds, batch_size = 5)\n",
    "\n",
    "\n",
    "valid_batch = datagen.flow_from_directory(valid_path, target_size = (224, 224),\n",
    "                                         classes = breeds, batch_size = 5)\n",
    "test_batch = datagen.flow_from_directory(test_path, target_size = (224, 224),\n",
    "                                        classes = breeds, batch_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning model\n",
    "vgg16_model = tf.keras.applications.vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Sequential Model out of the base VGG16 Model\n",
    "analyzer_model = Sequential()\n",
    "\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    analyzer_model.add(layer)\n",
    "    \n",
    "# Sequential Model almost the same as VGG16 model, but no input or predictions layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers that have already been trained\n",
    "for layer in analyzer_model.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add final prediction layer (dense) to predict the 120 classes\n",
    "analyzer_model.add(Dense(120, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               491640    \n",
      "=================================================================\n",
      "Total params: 134,752,184\n",
      "Trainable params: 17,272,952\n",
      "Non-trainable params: 117,479,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "analyzer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Resize images to be 244 x 244\n",
    "- Add predictions layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "optimizer = Adam(lr = 0.0001)\n",
    "analyzer_model.compile(optimizer = optimizer,\n",
    "                      loss = categorical_crossentropy,\n",
    "                      metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32 is good starting point\n",
    "# steps_per_epoch = training_size / batch_size\n",
    "analyzer_model.fit(train_batch1, batch_size = 32, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_model.fit(train_batch2, batch_size = 32, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_model.fit(train_batch3, batch_size = 32, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_model.fit(train_batch4, batch_size = 32, epochs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_model.fit(train_batch5, validation_batch = valid_batch, batch_size = 32, epochs = 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_TF2.0]",
   "language": "python",
   "name": "conda-env-py3_TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
